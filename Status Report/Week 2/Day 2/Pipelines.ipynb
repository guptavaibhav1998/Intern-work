{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Pipelines\n",
    "We need to handle the numerical and categorical attributes differently. Numerical attributes need to be scaled, whereas for categorical columns we need to fill the missing values and then encode the categorical values into numerical values. To apply these sequence of transformations we will use the sklearn’s Pipeline class. We will also build custom transformers that can be directly used with Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ColumnsSelector Pipeline\n",
    "sklearn doesn’t provide libraries to directly manipulate with pandas dataframe. We will write our own custom transformer which will select the corresponding attributes (either numerical or categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selects only few colums from our dataset\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "  def __init__(self, type):\n",
    "    self.type = type\n",
    "  \n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "  \n",
    "  def transform(self,X):\n",
    "    return X.select_dtypes(include=[self.type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Data Pipeline\n",
    "\n",
    "1. We select the numerical attributes using the ColumnsSelector transformer defined above and then scale the values using the StandardScaler.                                                                                                                \n",
    "2. Pipeline of transforms with a final estimator.                                                \n",
    "3. Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that    is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the    pipeline can be cached using memory argument.                                                            \n",
    "4. steps : list\n",
    "   List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with    the last object an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"num_attr_selector\", ColumnsSelector(type='int')),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Data Pipeline\n",
    "1. We need to replace the missing values in the categorical columns. We will replace the missing values with the most frequently occurring value in each column. sklearn comes with Imputer to handle missing values. \n",
    "2. However, Imputer works only with numerical values. We will write a custom transformer which will accept a list of columns for which we need to replace the missing values and the strategy used to fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalImputer(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "  def __init__(self, columns = None, strategy='most_frequent'):\n",
    "    self.columns = columns\n",
    "    self.strategy = strategy\n",
    "    \n",
    "    \n",
    "  def fit(self,X, y=None):\n",
    "    if self.columns is None:\n",
    "      self.columns = X.columns\n",
    "    \n",
    "    if self.strategy is 'most_frequent':\n",
    "      self.fill = {column: X[column].value_counts().index[0] for \n",
    "        column in self.columns}\n",
    "    else:\n",
    "      self.fill ={column: '0' for column in self.columns}\n",
    "      \n",
    "    return self\n",
    "      \n",
    "  def transform(self,X):\n",
    "    X_copy = X.copy()\n",
    "    for column in self.columns:\n",
    "      X_copy[column] = X_copy[column].fillna(self.fill[column])\n",
    "    return X_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the machine learning models expect numerical values. We will use pd.get_dummies to convert the categorical values to numerical values. This is similar to using OneHotEncoder except that OneHotEncoder requires numerical columns.\n",
    "\n",
    "We need to merge the train and test dataset before using pd.get_dummies as there might be classes in the test dataset that might not be present in the training dataset. \n",
    "\n",
    "For this, in the fit method, we will concatenate the train and test dataset and find out all the possible values for a column. \n",
    "\n",
    "In the transform method, we will convert each column to Categorical Type and specify the list of categories that the column can take. pd.get_dummies will create a column of all zeros for the category not present in the list of the categories for that column.\n",
    "\n",
    "The transformer also takes an argument dropFirst which indicates whether we should drop the first column after creating dummy columns using pd.get_dummies. We should drop the first column to avoid multicollinearity. By default, the value is set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "  def __init__(self, dropFirst=True):\n",
    "    self.categories=dict()\n",
    "    self.dropFirst=dropFirst\n",
    "    \n",
    "  def fit(self, X, y=None):\n",
    "    join_df = pd.concat([train_data, test_data])\n",
    "    join_df = join_df.select_dtypes(include=['object'])\n",
    "    for column in join_df.columns:\n",
    "      self.categories[column] = join_df[column].value_counts().index.tolist()\n",
    "    return self\n",
    "    \n",
    "  def transform(self, X):\n",
    "    X_copy = X.copy()\n",
    "    X_copy = X_copy.select_dtypes(include=['object'])\n",
    "    for column in X_copy.columns:\n",
    "      X_copy[column] = X_copy[column].astype(\n",
    "          {column:\n",
    "                CategoricalDtype(self.categories[column])\n",
    "          })\n",
    "    return pd.get_dummies(X_copy, drop_first=self.dropFirst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Categorical Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"cat_attr_selector\", ColumnsSelector(type='object')),\n",
    "    (\"cat_imputer\", CategoricalImputer(columns=\n",
    "          ['workClass','occupation', 'native-country'])),\n",
    "    (\"encoder\", CategoricalEncoder(dropFirst=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Pipeline\n",
    "\n",
    "We have two transformer pipeline i.e, num_pipeline and cat_pipeline. We can merge them using FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion([(\"num_pipe\", num_pipeline), (\"cat_pipeline\", cat_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
